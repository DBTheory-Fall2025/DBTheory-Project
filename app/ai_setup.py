import google.generativeai as genai
import os
from typing import Generator

genai.configure(api_key=os.getenv("API_KEY"))

system_prompt = (
    "You are a helpful SQL data engineering agent. You will be provided with two databases and your task is to combine them."
    "Always generate valid SQL queries, and explain briefly what each query does."
)

# Conversation history
_histories = {}

def model(user_prompt, agent_name="default"):
    """
    Non-streaming model call. Returns the complete response.
    Use this for backward compatibility.
    """
    if agent_name not in _histories:
        _histories[agent_name] = []

    _histories[agent_name].append(f"User: {user_prompt}")

    gm = genai.GenerativeModel("gemini-2.5-pro")
    response = gm.generate_content("\n".join(_histories[agent_name]))

    text = response.text
    _histories[agent_name].append(f"Assistant: {text}")
    return text


def model_stream(user_prompt, agent_name="default") -> Generator[str, None, None]:
    """
    Streaming model call. Yields tokens/chunks as they arrive from the API.
    
    Args:
        user_prompt: The prompt to send to the model
        agent_name: The agent name for conversation history tracking
    
    Yields:
        Chunks of text as they're generated by the model
    
    Returns:
        A generator that yields text chunks
    """
    if agent_name not in _histories:
        _histories[agent_name] = []

    _histories[agent_name].append(f"User: {user_prompt}")

    print(f"[MODEL_STREAM] {agent_name} - Creating streaming request to Gemini API", flush=True)
    
    gm = genai.GenerativeModel("gemini-2.5-pro")
    response = gm.generate_content(
        "\n".join(_histories[agent_name]),
        stream=True
    )

    complete_text = ""
    chunk_count = 0
    
    # Stream the response content
    print(f"[MODEL_STREAM] {agent_name} - Starting to iterate through response chunks", flush=True)
    for chunk in response:
        if chunk.text:
            chunk_count += 1
            complete_text += chunk.text
            print(f"[MODEL_STREAM] {agent_name} - Yielding chunk {chunk_count}: {len(chunk.text)} chars", flush=True)
            yield chunk.text
    
    print(f"[MODEL_STREAM] {agent_name} - Finished streaming {chunk_count} chunks, total {len(complete_text)} chars", flush=True)
    
    # Add complete response to history
    _histories[agent_name].append(f"Assistant: {complete_text}")
